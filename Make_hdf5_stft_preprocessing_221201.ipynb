{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler , minmax_scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import pandas as pd\n",
    "new_MITmon_matrix=['FP1-F7','F7-T7','T7-FT9','T7-P7','P7-O1','FP1-F3','F3-C3','FZ-CZ','C3-P3','P3-O1',\n",
    "                'FP2-F4', 'F4-C4','CZ-PZ','C4-P4','P4-O2','FP2-F8','F8-T8','FT10-T8','T8-P8','P8-O2' ]\n",
    "\n",
    "Mon_matrix=MITmon_matrix7\n",
    "Mon_matrix_col_no=4\n",
    "Mon_matrix_row_no=5 #row는 4로 고정\n",
    "\n",
    "sampling_rate=256 #MIT는 256, SNUH는 200\n",
    "freq_seg=0.5 #freq domain 기본 간격freq_total_n=sampling_rate/freq_seg # freqdomain 간격 총 개수\n",
    "freq_upper_range=40 #freq upper range Hz\n",
    "freq_n=int(freq_upper_range/freq_seg) #일단 현재는 40\n",
    "\n",
    "sec_pre_sz=40 #sz onset 전\n",
    "sec_post_sz=40 # sz onset 후\n",
    "sec_int_sz=sec_pre_sz+sec_post_sz\n",
    "\n",
    "sec_frame=20 #LSTM의 frame 길이\n",
    "\n",
    "ch_total_no = 20\n",
    "\n",
    "power_weight=1 #가중치\n",
    "with open('/MIT_data/source/pickle_file/information.pickle', 'rb') as f:\n",
    "    information_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mon_matrix = new_MITmon_matrix \n",
    "new_index_list = []\n",
    "new_Mon_matrix_col_no=4\n",
    "new_Mon_matrix_row_no=5\n",
    "\n",
    "for i in range(len(Mon_matrix)):  ## h5 파일에 data 집어넣을시 위치 지정할 list 생성         \n",
    "    a=4-(i % new_Mon_matrix_row_no) ## i%new_Mon_matrix_row_no의 의미 i와 new_Mon_matrix_row_no를 나누었을때의 나머지\n",
    "    b=int(i/new_Mon_matrix_row_no)\n",
    "    new_index_list.append([a,b,new_MITmon_matrix[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keylist=list(information_dict.keys())\n",
    "sz_dict = {}\n",
    "for j in range(0,24):   \n",
    "    sz_dict[j+1] = {}\n",
    "    start_list = []\n",
    "    sz_len_list = []\n",
    "    #h5f=h5py.File('/MIT_data/source/h5_file/stft/'+'chb'+str(keylist[j])+'_'+'data.h5','r')\n",
    "    #data=h5f['all_data/data']     \n",
    "   \n",
    "    for k in range(len(information_dict[keylist[j]]['new_sz_info_list'])): \n",
    "        sz_edf_num=information_dict[keylist[j]]['new_sz_info_list'][k][-1]       \n",
    "        before_time = 0\n",
    "        for ti in range(information_dict[keylist[j]]['new_sz_info_list'][k][-1]-1):\n",
    "            before_time = before_time + information_dict[keylist[j]]['time'][ti]\n",
    "        start = int(information_dict[keylist[j]]['new_sz_info_list'][k][0] + before_time)   \n",
    "        #print(start)\n",
    "        start_list.append(start)\n",
    "        sz_len_list.append(information_dict[keylist[j]]['new_sz_info_list'][k][1]-information_dict[keylist[j]]['new_sz_info_list'][k][0])\n",
    "    sz_dict[j+1]['start'] = start_list\n",
    "    sz_dict[j+1]['len'] = sz_len_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_n,time,Mon_matrix_row_no,Mon_matrix_col_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for order in range(1,2): \n",
    "    total_length_sec = 0\n",
    "    chb_sec = 0     \n",
    "    total_length_sec  = 0\n",
    "    total_length_sec=sum(information_dict[order]['time'])    \n",
    "    h5f=h5py.File('/MIT_data/source/h5_file/stft_data/chb'+str(order)+'_data.h5', 'a') \n",
    "    h5f.create_group('all_data')\n",
    "    h5f.create_dataset('all_data/data', (freq_n,total_length_sec,Mon_matrix_row_no,Mon_matrix_col_no), compression=\"lzf\")#   \n",
    "    len_score=0  \n",
    "    pickle_list=os.listdir('/MIT_data/source/pickle_file/stft_filter/'+str(order))##각 환자별 pickle list \n",
    "    for x in range(0,len(pickle_list)):## 딥러닝 모델에 맞게 데이터 형식 변형         \n",
    "        with open('/MIT_data/source/pickle_file/stft_filter/'+str(order)+'/'+pickle_list[x], 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        len_=len(data[MITmon_matrix7[1]][0])        \n",
    "        time=len_  # EDF 파일 전체에서 FFT 하고 남는 마지막 하나 수치를 빼기 위해.   \n",
    "\n",
    "        channel_location_data=[]\n",
    "        try_=np.zeros((freq_n,time,Mon_matrix_row_no,Mon_matrix_col_no))\n",
    "        input_zero = np.zeros((freq_n,time))           \n",
    "        score = 0\n",
    "        for i in range(len(new_index_list)):\n",
    "            a=new_index_list[i][0]\n",
    "            b=new_index_list[i][1]\n",
    "            if new_index_list[i][-1] == -1:\n",
    "                try_[:,:,a,b] = input_zero\n",
    "            else:                \n",
    "                try:\n",
    "                    try_[:,:,a,b] = data[new_index_list[i][-1]]\n",
    "                except:\n",
    "                    try_[:,:,a,b] = data[new_index_list[i][-1]][:,:-1] \n",
    "                            \n",
    "\n",
    "        channel_location_data = np.array(try_)               \n",
    "        len_score = len_score + len(channel_location_data[0]) ## 반복문이 돌아가면서 들어갈 위치 지정       \n",
    "\n",
    "                       \n",
    "        h5f['all_data/data'][:,len_score-len(channel_location_data[0]):len_score,:] = channel_location_data \n",
    "       \n",
    "          \n",
    "\n",
    "        print('len_score:'+str(len_score))      \n",
    "       \n",
    "    h5f.close()                \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## label 부분 만드는 코드\n",
    "for order in range(1,2): \n",
    "    pickle_list=os.listdir('pickle_file/stft_filter/'+str(order))##각 환자별 pickle list \n",
    "    len_score = 0\n",
    "    h5f=h5py.File('h5_file/label_data_/chb'+str(order)+'_data.h5', 'a') \n",
    "    h5f.create_group('all_data')\n",
    "    h5f.create_dataset('all_data/label', (total_length_sec,2), compression=\"lzf\") ## lable index 생성\n",
    "    for x in range(0,len(pickle_list)):\n",
    "        with open('pickle_file/stft_filter/'+str(order)+'/'+pickle_list[x], 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        len_score = len_score + len(data['FP1-F7'][0])## label의 길이를 맞추기 위해 가져온다\n",
    "        train_y = []    \n",
    "        make_real = []   \n",
    "        match_dict = {}\n",
    "        for edf in range(len(information_dict[order]['new_sz_info_list'])):\n",
    "            if pickle_list[x].split('.')[0] == str(information_dict[order]['new_sz_info_list'][edf][-1]).zfill(2):                \n",
    "                ## 하나의 pickle 파일안에 두개 이상의 seizure이 있을 경우가 있기 때문에 match_dict에 key가 필요함\n",
    "                match_dict[edf] = {}\n",
    "                match_dict[edf] = edf       \n",
    "\n",
    "        ## seizure 부분이 있는 경우\n",
    "        if match_dict != {}:            \n",
    "            match_key=list(match_dict.keys())\n",
    "            print('-----------------------------')\n",
    "            ## 지정된 길이만큼 0의 리스트를 만든다\n",
    "            for i in range(0,len(data['FP1-F7'][0])):\n",
    "                make_real.append(0)\n",
    "            ## 만들어진 리스트에 seizure 부분에 1을 집어 넣는다    \n",
    "            for k in range(len(match_dict)):                                 \n",
    "                for i in range(information_dict[order]['new_sz_info_list'][match_key[k]][0],information_dict[order]['new_sz_info_list'][match_key[k]][1]):\n",
    "                    make_real[i] =1     \n",
    "            make_real = np.array(make_real)\n",
    "            ## keras에 있는 모듈을 사용하여 category data를 만든다\n",
    "            cate_y=to_categorical(make_real,num_classes=2)\n",
    "            \n",
    "            h5f['all_data/label'][len_score-len(cate_y):len_score]= cate_y\n",
    "        ## seizure 부분이 없는 경우\n",
    "        else:          \n",
    "            ## 모든 부분을 0을 채우고 그대로 진행\n",
    "            for i in range(0,len(data['FP1-F7'][0])):\n",
    "                make_real.append(0)                      \n",
    "            make_real = np.array(make_real)\n",
    "            \n",
    "            ## keras에 있는 모듈을 사용하여 category data를 만든다\n",
    "            cate_y=to_categorical(make_real,num_classes=2)\n",
    "\n",
    "\n",
    "            h5f['all_data/label'][len_score-len(cate_y):len_score]= cate_y## 지정된 위치에 데이터를 집어넣는다\n",
    "\n",
    "        print('len_score:'+str(len_score))      ## 들어가는 위치 확인  \n",
    "\n",
    "    h5f.close() ## 들어가고 h5f 파일 손상을 막기위해 닫는다.                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#se_dict = {}\n",
    "weight_normal_dict= {}\n",
    "\n",
    "for order in range(0,24):   \n",
    "    num =  order + 1\n",
    "    weight_normal_dict[num] = []\n",
    "  \n",
    "    #se_dict[num] = {}\n",
    "    print(num)    \n",
    "    #h5f=h5py.File('h5_file/stft_filter_60/'+'chb'+str(num)+'_'+'data.h5','r')\n",
    "    label=h5py.File('h5_file/label_data/'+'chb'+str(num)+'_'+'data.h5','r')\n",
    "    total_length_sec = len(label['all_data/label']) \n",
    "    for i in range(total_length_sec):\n",
    "        if label['all_data/label'][i][0] == 0:\n",
    "            weight_normal_dict[num].append(se_dict[num][0]/sum(se_dict[num]) * 100)\n",
    "           \n",
    "        else:            \n",
    "            weight_normal_dict[num].append(se_dict[num][1]/sum(se_dict[num]) * 100)\n",
    "    with open('/MIT_data/source/pickle_file/weight_dict.pickle', 'wb') as f:\n",
    "        pickle.dump(freq_dict, f, pickle.HIGHEST_PROTOCOL)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
