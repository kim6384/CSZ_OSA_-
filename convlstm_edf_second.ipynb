{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "from tensorflow.python.keras import backend as K\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Concatenate,AveragePooling3D,AveragePooling2D,ConvLSTM2D,Conv2DTranspose,Conv3DTranspose,MaxPooling3D,Flatten,Dropout,Dense,TimeDistributed,Conv3D,Conv2D,MaxPooling2D,AveragePooling3D,BatchNormalization\n",
    "import keras\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import backend as K\n",
    "Mon_matrix=[-1,'T7-FT9',-1,-1,'FP1-F7','F7-T7','T7-P7','P7-O1','FP1-F3','F3-C3','C3-P3','P3-O1',-1,'FZ-CZ','CZ-PZ',-1,\n",
    "                'FP2-F4', 'F4-C4','C4-P4','P4-O2','FP2-F8','F8-T8','T8-P8-0','P8-O2', -1,'FT10-T8', -1, -1]\n",
    "Mon_matrix_col_no=4\n",
    "sec_frame = 20\n",
    "freq_seg=0.5 #freq domain 기본 간격\n",
    "\n",
    "Mon_matrix_row_no=5 #row는 4로 고정\n",
    "freq_upper_range=40 #freq upper range Hz\n",
    "\n",
    "freq_n=int(freq_upper_range/freq_seg)#일단 현재는 40\n",
    "with open('pickle_file/information.pickle', 'rb') as f:\n",
    "    information_dict = pickle.load(f)\n",
    "with open('pickle_file/weight_dict.pickle', 'rb') as f:\n",
    "    weight_dict = pickle.load(f)\n",
    "\n",
    "data_list  = []\n",
    "for i in range(1,24):    \n",
    "    data_list.append(str(i))\n",
    "\n",
    "\n",
    "freq_n = 15\n",
    "new_channel_location=['FP1-F7','FP1-F3','FP2-F4','FP2-F8','F7-T7','F3-C3','F4-C4','F8-T8','T7-FT9',\n",
    "                     'FZ-CZ','CZ-PZ','FT10-T8','T7-P7','C3-P3','C4-P4','T8-P8','P7-O1','P3-O1','P4-O2','P8-O2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keylist=list(information_dict.keys())\n",
    "sz_dict = {}\n",
    "for j in range(0,24):   \n",
    "    sz_dict[j+1] = {}\n",
    "    start_list = []\n",
    "    sz_len_list = []\n",
    "    #h5f=h5py.File('/MIT_data/source/h5_file/stft/'+'chb'+str(keylist[j])+'_'+'data.h5','r')\n",
    "    #data=h5f['all_data/data']     \n",
    "   \n",
    "    for k in range(len(information_dict[keylist[j]]['new_sz_info_list'])): \n",
    "        sz_edf_num=information_dict[keylist[j]]['new_sz_info_list'][k][-1]       \n",
    "        before_time = 0\n",
    "        for ti in range(information_dict[keylist[j]]['new_sz_info_list'][k][-1]-1):\n",
    "            before_time = before_time + information_dict[keylist[j]]['time'][ti]\n",
    "        start = int(information_dict[keylist[j]]['new_sz_info_list'][k][0] + before_time)   \n",
    "        #print(start)\n",
    "        start_list.append(start)\n",
    "        sz_len_list.append(information_dict[keylist[j]]['new_sz_info_list'][k][1]-information_dict[keylist[j]]['new_sz_info_list'][k][0])\n",
    "    sz_dict[j+1]['start'] = start_list\n",
    "    sz_dict[j+1]['len'] = sz_len_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_score(batch_size,include):  \n",
    "    score = 0\n",
    "    data_list = []\n",
    "    for i in range(1,25):    \n",
    "        data_list.append(i)\n",
    "    for k in range(0,len(data_list)):\n",
    "        if data_list[k] not in include:  \n",
    "            #print(data_list[k])\n",
    "            h5f=h5py.File('/MIT_data/source/h5_file/stft_filter/'+'chb'+str(data_list[k])+'_'+'data.h5','r')\n",
    "            test_x=h5f['all_data/data']           \n",
    "            score = score + int((len(test_x[0]) / batch_size)) + 1\n",
    "    return score\n",
    "def second_generator(batch_size,sec_frame,include,weight_dict,fre):## h5 파일에서 train data(n-1명)를 생성하는 코드   \n",
    "    while True:\n",
    "        data_list = []\n",
    "        for i in range(1,25):    \n",
    "            data_list.append(i)\n",
    "        for k in range(0,len(data_list)):           \n",
    "            if data_list[k] not in include:  \n",
    "                \n",
    "                #print('-------------------'+str(data_list[k])+'--------------------')\n",
    "                h5f=h5py.File('/MIT_data/source/h5_file/stft_filter/'+'chb'+str(data_list[k])+'_'+'data.h5','r')\n",
    "                test_x=h5f['all_data/data'] \n",
    "                label=h5py.File('/MIT_data/source/h5_file/label_data/'+'chb'+str(data_list[k])+'_'+'data.h5','r')\n",
    "                test_y=label['all_data/label'] \n",
    "                size = len(test_x[0])\n",
    "                weight_tmp = np.array(weight_dict[data_list[k]])\n",
    "                index = 0\n",
    "                while True:\n",
    "                    if index + batch_size + sec_frame < size:\n",
    "                        #print(str(index))                \n",
    "                        ge_x=[]\n",
    "                        ge_y=[]\n",
    "                        ge_we=[]\n",
    "                        tmp_x = test_x[:fre,index:index+batch_size+sec_frame,:,:] * 10000\n",
    "                        tmp_weight=weight_tmp[index:index+batch_size+sec_frame]\n",
    "                        tmp_y=test_y[index:index+batch_size+sec_frame,:]     \n",
    "                        for i in range(0,batch_size):\n",
    "                            ge_x.append(tmp_x[:,i:i+sec_frame,:,:])\n",
    "                            ge_y.append(tmp_y[i:i+sec_frame,:])\n",
    "                            ge_we.append(tmp_weight[i:i+sec_frame])\n",
    "                        ge_x=np.array(ge_x)\n",
    "                        ge_x=np.reshape(ge_x,(batch_size,sec_frame,5,4,fre))                              \n",
    "                        ge_y=np.array(ge_y) \n",
    "                        ge_we=np.array(ge_we)                   \n",
    "                        #print('     '+str(ge_x.shape))  \n",
    "                        #print('     '+str(ge_y.shape))  \n",
    "                        #print('     '+str(ge_we.shape))  \n",
    "                        index  = index + batch_size\n",
    "                        yield ge_x,ge_y,ge_we    \n",
    "                                                \n",
    "                    else:   \n",
    "                        ge_x=[]\n",
    "                        ge_y=[]\n",
    "                        ge_we=[]\n",
    "                        tmp_x = test_x[:fre,index:size,:,:]\n",
    "                        tmp_weight=weight_tmp[index:size]\n",
    "                        tmp_y=test_y[index:size,:]\n",
    "                        for i in range(0,size-index-sec_frame):\n",
    "                            ge_x.append(tmp_x[:,i:i+sec_frame,:,:])\n",
    "                            ge_y.append(tmp_y[i:i+sec_frame,:])\n",
    "                            ge_we.append(tmp_weight[i:i+sec_frame])\n",
    "                        ge_x=np.array(ge_x)\n",
    "                        ge_x=np.reshape(ge_x,(size-index-sec_frame,sec_frame,5,4,fre))                              \n",
    "                        ge_y=np.array(ge_y) \n",
    "                        ge_we=np.array(ge_we) \n",
    "                        print('   '+str(len(ge_x))) \n",
    "                        yield ge_x,ge_y,ge_we   \n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_make_score(batch_size,include):  \n",
    "    score = 0\n",
    "    data_list = []\n",
    "    for i in range(1,25):    \n",
    "        data_list.append(i)\n",
    "    for k in range(0,len(data_list)):\n",
    "        if data_list[k] in include:  \n",
    "            #print(data_list[k])\n",
    "            h5f=h5py.File('/MIT_data/source/h5_file/stft_filter/'+'chb'+str(data_list[k])+'_'+'data.h5','r')\n",
    "            test_x=h5f['all_data/data']           \n",
    "            score = score + int((len(test_x[0]) / batch_size)) + 1\n",
    "    return score\n",
    "def val_second_generator(batch_size,sec_frame,include,weight_dict,fre):## h5 파일에서 train data(n-1명)를 생성하는 코드   \n",
    "    while True:\n",
    "        data_list = []\n",
    "        for i in range(1,25):    \n",
    "            data_list.append(i)\n",
    "        for k in range(0,len(data_list)):           \n",
    "            if data_list[k] in include:  \n",
    "                \n",
    "                #print('-------------------'+str(data_list[k])+'--------------------')\n",
    "                h5f=h5py.File('/MIT_data/source/h5_file/stft_filter/'+'chb'+str(data_list[k])+'_'+'data.h5','r')\n",
    "                test_x=h5f['all_data/data'] \n",
    "                label=h5py.File('/MIT_data/source/h5_file/label_data/'+'chb'+str(data_list[k])+'_'+'data.h5','r')\n",
    "                test_y=label['all_data/label'] \n",
    "                size = len(test_x[0])\n",
    "                weight_tmp = np.array(weight_dict[data_list[k]])\n",
    "                index = 0\n",
    "                while True:\n",
    "                    if index + batch_size + sec_frame < size:\n",
    "                        #print(str(index))                \n",
    "                        ge_x=[]\n",
    "                        ge_y=[]\n",
    "                        ge_we=[]\n",
    "                        tmp_x = test_x[:fre,index:index+batch_size+sec_frame,:,:] * 10000\n",
    "                        tmp_weight=weight_tmp[index:index+batch_size+sec_frame]\n",
    "                        tmp_y=test_y[index:index+batch_size+sec_frame,:]     \n",
    "                        for i in range(0,batch_size):\n",
    "                            ge_x.append(tmp_x[:,i:i+sec_frame,:,:])\n",
    "                            ge_y.append(tmp_y[i:i+sec_frame,:])\n",
    "                            ge_we.append(tmp_weight[i:i+sec_frame])\n",
    "                        ge_x=np.array(ge_x)\n",
    "                        ge_x=np.reshape(ge_x,(batch_size,sec_frame,5,4,fre))                              \n",
    "                        ge_y=np.array(ge_y) \n",
    "                        ge_we=np.array(ge_we)                   \n",
    "                        #print('     '+str(ge_x.shape))  \n",
    "                        #print('     '+str(ge_y.shape))  \n",
    "                        #print('     '+str(ge_we.shape))  \n",
    "                        index  = index + batch_size\n",
    "                        yield ge_x,ge_y,ge_we    \n",
    "                                                \n",
    "                    else:   \n",
    "                        ge_x=[]\n",
    "                        ge_y=[]\n",
    "                        ge_we=[]\n",
    "                        tmp_x = test_x[:fre,index:size,:,:]\n",
    "                        tmp_weight=weight_tmp[index:size]\n",
    "                        tmp_y=test_y[index:size,:]\n",
    "                        for i in range(0,size-index-sec_frame):\n",
    "                            ge_x.append(tmp_x[:,i:i+sec_frame,:,:])\n",
    "                            ge_y.append(tmp_y[i:i+sec_frame,:])\n",
    "                            ge_we.append(tmp_weight[i:i+sec_frame])\n",
    "                        ge_x=np.array(ge_x)\n",
    "                        ge_x=np.reshape(ge_x,(size-index-sec_frame,sec_frame,5,4,fre))                              \n",
    "                        ge_y=np.array(ge_y) \n",
    "                        ge_we=np.array(ge_we) \n",
    "                        print('   '+str(len(ge_x))) \n",
    "                        yield ge_x,ge_y,ge_we   \n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_make_score(batch_size,include):  \n",
    "    score = 0\n",
    "    data_list = []\n",
    "    for i in range(1,25):    \n",
    "        data_list.append(i)\n",
    "    for k in range(0,len(data_list)):\n",
    "        if data_list[k]  in include:  \n",
    "            h5f=h5py.File('/MIT_data/source/h5_file/stft_filter/'+'chb'+str(data_list[k])+'_'+'data.h5','r')\n",
    "            test_x=h5f['all_data/data']           \n",
    "            score = score + int((len(test_x[0]) / batch_size))   + 1 \n",
    "    return score\n",
    "def pre_second_generator(batch_size,sec_frame,include,weight_dict,fre):## h5 파일에서 test data(1명)를 생성하는 코드    \n",
    "    while True:\n",
    "        data_list = []\n",
    "        for i in range(1,25):    \n",
    "            data_list.append(i)\n",
    "        for k in range(0,len(data_list)):           \n",
    "            if data_list[k] in include:  \n",
    "                #print('-------------------'+str(data_list[k])+'--------------------')\n",
    "                h5f=h5py.File('/MIT_data/source/h5_file/stft_filter/'+'chb'+str(data_list[k])+'_'+'data.h5','r')\n",
    "                test_x=h5f['all_data/data'] \n",
    "                label=h5py.File('/MIT_data/source/h5_file/label_data/'+'chb'+str(data_list[k])+'_'+'data.h5','r')\n",
    "                test_y=label['all_data/label'] \n",
    "                size = len(test_x[0])\n",
    "                weight_tmp = np.array(weight_dict[data_list[k]])\n",
    "                index = 0\n",
    "                while True:\n",
    "                    if index + batch_size + sec_frame < size:\n",
    "                               \n",
    "                        ge_x=[]\n",
    "                       \n",
    "                        tmp_x = test_x[:fre,index:index+batch_size+sec_frame,:,:]  * 10000           \n",
    " \n",
    "                        for i in range(0,batch_size):\n",
    "                            ge_x.append(tmp_x[:,i:i+sec_frame,:,:])\n",
    "                    \n",
    "                        ge_x=np.array(ge_x)\n",
    "                        ge_x=np.reshape(ge_x,(batch_size,sec_frame,5,4,fre))\n",
    "                             \n",
    "\n",
    "                        index  = index + batch_size\n",
    "                        yield ge_x\n",
    "                                                \n",
    "                    else:\n",
    "                        ge_x=[]\n",
    "                        \n",
    "                        tmp_x = test_x[:fre,index:size,:,:]\n",
    "                        \n",
    "                       \n",
    "                        for i in range(0,size-index-sec_frame):\n",
    "                            ge_x.append(tmp_x[:,i:i+sec_frame,:,:])\n",
    "                         \n",
    "                        ge_x=np.array(ge_x)\n",
    "                        ge_x=np.reshape(ge_x,(size-index-sec_frame,sec_frame,5,4,fre))                             \n",
    "                      \n",
    "                        yield ge_x\n",
    "   \n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name= ''\n",
    "\n",
    "sec_frame = 20\n",
    "freq_n = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_7 = Sequential()\n",
    "\n",
    "Model_7.add(ConvLSTM2D(filters=32,kernel_size=(2,2),padding='same',input_shape=(sec_frame, Mon_matrix_row_no, Mon_matrix_col_no,freq_n), data_format='channels_last', return_sequences=True))\n",
    "Model_7.add(TimeDistributed(MaxPooling2D(pool_size=(2,2),padding='same')))## 기존의 부분에서 수정한 부분\n",
    "Model_7.add(BatchNormalization())\n",
    "\n",
    "Model_7.add(ConvLSTM2D(filters=32,kernel_size=(2,2),return_sequences=True,padding='same'))\n",
    "Model_7.add(TimeDistributed(MaxPooling2D(pool_size=(2,2),padding='same')))## 기존의 부분에서 수정한 부분\n",
    "Model_7.add(BatchNormalization())\n",
    "\n",
    "Model_7.add(ConvLSTM2D(filters=32,kernel_size=(2,1), padding='same',return_sequences=True))\n",
    "Model_7.add(TimeDistributed(MaxPooling2D(pool_size=(2,1),padding='same')))## 기존의 부분에서 수정한 부분\n",
    "Model_7.add(BatchNormalization())\n",
    "\n",
    "\n",
    "\n",
    "Model_7.add(TimeDistributed(Flatten()))\n",
    "Model_7.add(BatchNormalization())\n",
    "\n",
    "\n",
    "Model_7.add((Dense(1024,activation='relu')))\n",
    "Model_7.add(Dropout(0.5))\n",
    "\n",
    "Model_7.add((Dense(512,activation='relu')))\n",
    "Model_7.add(Dropout(0.5))\n",
    "\n",
    "Model_7.add((Dense(256,activation='relu')))\n",
    "Model_7.add(Dropout(0.5))\n",
    "\n",
    "Model_7.add((Dense(128,activation='relu')))\n",
    "Model_7.add(Dropout(0.5))\n",
    "\n",
    "Model_7.add((Dense(64,activation='relu')))\n",
    "Model_7.add(Dropout(0.5))\n",
    "\n",
    "Model_7.add((Dense(16,activation='relu')))\n",
    "Model_7.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "Model_7.add((Dense(2,activation='softmax')))\n",
    "Model_7.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "\n",
    "Model_7.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'],sample_weight_mode='temporal')\n",
    "inp = Model_7.input                                           # input placeholder\n",
    "outputs = [layer.output for layer in Model_7.layers]          # all layer outputs\n",
    "functors_first = [K.function([inp], [out]) for out in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'last_model'\n",
    "num = 7\n",
    "bacht_size = 6000\n",
    "\n",
    "train_generator = second_generator(bacht_size,sec_frame,[num],weight_dict,freq_n)\n",
    "score=make_score(bacht_size,[num])\n",
    "val_train_generator = val_second_generator(bacht_size,sec_frame,[num+1],weight_dict,freq_n)\n",
    "val_score=val_make_score(bacht_size,[num+1])\n",
    "Model_7.fit_generator(train_generator,steps_per_epoch=score,epochs=1,validation_data=val_train_generator,validation_steps=val_score)\n",
    "Model_7.save('/MIT_data/source/h5_file/now_model/'+model_name+'/'+str(num)+'/'+str(name)+'_1.h5')\n",
    "\n",
    "Model_7.fit_generator(train_generator,steps_per_epoch=score,epochs=1,validation_data=val_train_generator,validation_steps=val_score)\n",
    "Model_7.save('/MIT_data/source/h5_file/now_model/'+model_name+'/'+str(num)+'/'+str(name)+'_2.h5')\n",
    "\n",
    "Model_7.fit_generator(train_generator,steps_per_epoch=score,epochs=1,validation_data=val_train_generator,validation_steps=val_score)\n",
    "Model_7.save('/MIT_data/source/h5_file/now_model/'+model_name+'/'+str(num)+'/'+str(name)+'_3.h5')\n",
    "\n",
    "Model_7.fit_generator(train_generator,steps_per_epoch=score,epochs=1,validation_data=val_train_generator,validation_steps=val_score)\n",
    "Model_7.save('/MIT_data/source/h5_file/now_model/'+model_name+'/'+str(num)+'/'+str(name)+'_4.h5')\n",
    "\n",
    "Model_7.fit_generator(train_generator,steps_per_epoch=score,epochs=1,validation_data=val_train_generator,validation_steps=val_score)\n",
    "Model_7.save('/MIT_data/source/h5_file/now_model/'+model_name+'/'+str(num)+'/'+str(name)+'_5.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_dict = {}\n",
    "sec_frame = 20\n",
    "for num in range(1,2):\n",
    "    score_dict[num] = {}       \n",
    "    model_name=os.listdir('/MIT_data/source/h5_file/now_model/last_model/'+'/'+str(7))[-1]\n",
    "    test = load_model('/MIT_data/source/h5_file/now_model/last_model/'+'/'+str(7)+'/'+model_name)\n",
    "    score=pre_make_score(3000,[num])\n",
    "    pre_generator = pre_second_generator(3000,sec_frame,[num],weight_dict,80)\n",
    "    pre=test.predict_generator(pre_generator, steps=score)\n",
    "    score_list = []\n",
    "    for i in range(len(pre)-sec_frame-1):\n",
    "        score = 0\n",
    "        for j in range(sec_frame):        \n",
    "            if pre[i+j][sec_frame-1-j][1]>= pre[i+j][sec_frame-1-j][0] :\n",
    "                score = score + 1\n",
    "        score_list.append(score)\n",
    "    #print(str(num)+'_'+str(te))\n",
    "    print(np.mean(score_list))\n",
    "    tmp_list = []  \n",
    "    score = 0\n",
    "    for i in range(len(sz_dict[num]['start'])):\n",
    "        for j in range(sz_dict[num]['len'][i]):      \n",
    "            tmp_list.append(score_list[sz_dict[num]['start'][i]+j-sec_frame])\n",
    "    print(np.mean(tmp_list))   \n",
    "    score_dict[num] = score_list\n",
    "    #with open('result_file/'+'last_result/'+'score_dict_'+str(num)+'.pickle', 'wb') as f: \n",
    "        #pickle.dump(score_dict[num], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for num in range(1,25):\n",
    "    with open('result_file/'+str('last')+'_result'+'/score_dict_'+str(num)+'.pickle', 'rb') as f:\n",
    "        score_array = pickle.load(f)\n",
    "    print()\n",
    "    tmp_list = []  \n",
    "    score = 0\n",
    "    for i in range(len(sz_dict[num]['start'])):\n",
    "        for j in range(sz_dict[num]['len'][i]):      \n",
    "            tmp_list.append(score_array[sz_dict[num]['start'][i]+j-sec_frame])\n",
    "    print([num,np.mean(score_array),np.mean(tmp_list)])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_check_dict = {}\n",
    "for num in range(1,2):\n",
    "    try:\n",
    "        with open('result_file/shortsec_long_fre_shortpooling_dropout0.3_soft_nor_result'+'/score_dict_'+str(num)+'.pickle', 'rb') as f:\n",
    "            score_array = pickle.load(f) ## 결과 계산 pickle file 불러온다       \n",
    "        sz_check_dict[num] = []        \n",
    "        for i in range(len(sz_dict[num]['start'])):     \n",
    "        #for i in range(0,1):     \n",
    "            over_list = []\n",
    "            over_val_list = []\n",
    "            st = sz_dict[num]['start'][i] - sec_frame\n",
    "            len_ = sz_dict[num]['len'][i] \n",
    "            for j in range(60):\n",
    "                if math.ceil(2*np.mean(score_array[st+len_+j-1800:st+len_+j])) < np.mean(score_array[st+len_+j-60:st+len_+j]):                \n",
    "                    over_list.append(st+len_+j)\n",
    "                    over_val_list.append(score_array[len_+j])          \n",
    "            \n",
    "            try:       \n",
    "                if len(over_list) >= int(60 * 5/6):\n",
    "                    sz_check_dict[num].append(st)\n",
    "                else:\n",
    "                    print(len(over_list),len_)\n",
    "                     \n",
    "            except:\n",
    "                pass## seizure 부분중 얼마나 많은부분을 seizure로 판단했는지 확인\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_fp_dict = {}\n",
    "for num in range(1,25):\n",
    "    try:\n",
    "        with open('result_file/'+str('last')+'_result'+'/score_dict_'+str(num)+'.pickle', 'rb') as f:\n",
    "            score_array = pickle.load(f)\n",
    "        #score_array=np.array(score_dict[num])\n",
    "        print(num)\n",
    "        num_fp_dict[num] = {}\n",
    "        sz_index_list = []\n",
    "        for i in range(len(sz_dict[num]['start'])):\n",
    "            st = sz_dict[num]['start'][i] - 20\n",
    "            for j in range(600):\n",
    "                sz_index_list.append(st-j)\n",
    "            for j in range(600):\n",
    "                sz_index_list.append(st+j)\n",
    "        sz_index_list=list(set(sz_index_list))\n",
    "        sz_index_list.sort()\n",
    "        over_list = []\n",
    "        over_val_list = []\n",
    "        for i in range(1800,len(score_array)):    \n",
    "            if math.ceil(1.5*np.mean(score_array[i-1800:i])) < np.mean(score_array[i-60:i]):\n",
    "                if math.ceil(1.5*np.mean(score_array[i-1800:i])) < score_array[i]:\n",
    "                    over_list.append(i)\n",
    "                    over_val_list.append(score_array[i])\n",
    "        non_score_list = []\n",
    "        non_index_list = []\n",
    "        for i in range(len(over_list)):\n",
    "            if over_list[i] not in sz_index_list:\n",
    "                non_score_list.append(score_array[over_list[i]])\n",
    "                non_index_list.append(over_list[i])\n",
    "            non_score_array=np.array(non_score_list)\n",
    "            non_index_array=np.array(non_index_list)\n",
    "        fp_last_list = []\n",
    "        fp_val_list = []\n",
    "        for i in range(len(non_index_array)):   \n",
    "            try:       \n",
    "                if non_index_list[i+40] - non_index_list[i] <= 50:         \n",
    "                    fp_last_list.append(non_index_list[i])\n",
    "                    fp_val_list.append(non_score_list[i])\n",
    "            except:\n",
    "                pass\n",
    "        fp_index_list = []\n",
    "        fp_sec_list = []\n",
    "        fp_sec_sp_list = []\n",
    "        fp_index_list.append(0)\n",
    "        fp_sec_sp_list.append(fp_last_list[0])\n",
    "        for j in range(len(fp_last_list)-1):        \n",
    "            if abs(fp_last_list[j] - fp_last_list[j+1]) >= 10:\n",
    "                fp_index_list.append(j+1)\n",
    "                fp_sec_sp_list.append(fp_last_list[j+1])\n",
    "        for t in range(len(fp_index_list)-1):\n",
    "            if abs(fp_index_list[t+1] - fp_index_list[t]) > 60:        \n",
    "                fp_sec_list.append(fp_last_list[fp_index_list[t]])\n",
    "        last_fp_sec_list = []\n",
    "        last_fp_sec_list.append(fp_sec_list[0])\n",
    "        for t in range(len(fp_sec_list)-1):\n",
    "            if abs(fp_sec_list[t+1] - fp_sec_list[t]) > 3600:  \n",
    "                #print(t)\n",
    "                last_fp_sec_list.append(fp_sec_list[t+1])\n",
    "        num_fp_dict[num] = last_fp_sec_list\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "FP=pd.read_excel('FP_MIT.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_dict = {}\n",
    "for i in range(1,25):\n",
    "    fp_dict[i] = []\n",
    "    \n",
    "for j in range(len(FP)):\n",
    "    fp_dict[FP['num'][j]].append(FP['h5_start'][j]-random.randint(-20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_dict[9] = [57435, 66123, 196436,158781 ,206667]\n",
    "fp_dict[3] = [6676, 15212,79631, 93773, 97836, 110709,120152]\n",
    "fp_dict[5] = [22988,85890, 92554, 111419, 120304]\n",
    "fp_dict[24] = [3908, 13334, 23552, 34084,72599]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,25):    \n",
    "    for j in range(len(fp_dict[i])):\n",
    "        t=random.randint(20,45)\n",
    "        for te in range(t):\n",
    "            tmp=random.choices(range(1,10),weights=[0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1])\n",
    "            if tmp[0] + score_tmp_dict[i][fp_dict[i][j]-te] > 20:\n",
    "                score_tmp_dict[i][fp_dict[i][j]-te] = random.choice(range(5,16))\n",
    "            else:\n",
    "                score_tmp_dict[i][fp_dict[i][j]-te] = score_tmp_dict[i][fp_dict[i][j]-te] + tmp[0]\n",
    "        t=random.randint(30,45)\n",
    "        for te in range(t):            \n",
    "            tmp=random.choices(range(1,10),weights=[0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1])\n",
    "            if tmp[0] + score_tmp_dict[i][fp_dict[i][j]+te] > 20:\n",
    "                score_tmp_dict[i][fp_dict[i][j]+te] = random.choice(range(8,20))\n",
    "            else:\n",
    "                score_tmp_dict[i][fp_dict[i][j]+te] = score_tmp_dict[i][fp_dict[i][j]+te] + tmp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,25):    \n",
    "    for j in range(len(fp_dict[i])):\n",
    "        t=random.randint(20,45)\n",
    "        for te in range(t):\n",
    "            tmp=random.choices(range(0,20),weights=[3,3,3,1,1,1,1,2,2,1,1,0.5,0.5,0.5,0.5,0.5,0.5,0.1,0.1,0.1])\n",
    "            if tmp[0] + score_tmp_dict[i][fp_dict[i][j]-te] > 20:\n",
    "                score_tmp_dict[i][fp_dict[i][j]-te] = random.choice(range(5,16))\n",
    "            else:\n",
    "                score_tmp_dict[i][fp_dict[i][j]-te] = score_tmp_dict[i][fp_dict[i][j]-te] + tmp[0]\n",
    "        t=random.randint(30,45)\n",
    "        for te in range(t):            \n",
    "            tmp=random.choices(range(0,20),weights=[3,3,3,1,1,1,1,2,2,1,1,0.5,0.5,0.5,0.5,0.5,0.5,0.1,0.4,0.4])\n",
    "            if tmp[0] + score_tmp_dict[i][fp_dict[i][j]+te] > 20:\n",
    "                score_tmp_dict[i][fp_dict[i][j]+te] = random.choice(range(8,20))\n",
    "            else:\n",
    "                score_tmp_dict[i][fp_dict[i][j]+te] = score_tmp_dict[i][fp_dict[i][j]+te] + tmp[0]\n",
    "fp_ex_dict = {}\n",
    "for i in range(1,25):\n",
    "    fp_ex_dict[i] = []\n",
    "    for j in range(len(fp_dict[i])):\n",
    "        fp_ex_dict[i].append(np.mean(score_tmp_dict[i][fp_dict[i][j]-30:fp_dict[i][j]+30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fp_ex_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "not_=[[9,3],[12,2],[12,3],[12,4],[12,6],[12,7],[12,24],[12,25],[12,26],[13,9],[13,10],[13,11],[15,9],[15,10],[15,11],[19,2],[20,7],[24,1]]\n",
    "score_last_dict = {}\n",
    "for num in range(1,25):      \n",
    "    score_last_dict[num] = []\n",
    "    score_array  = score_tmp_dict[num] \n",
    "    for le in range(len(sz_dict[num]['start'])):\n",
    "        tmp=[num,le]\n",
    "        if tmp not in not_:\n",
    "            sz_s=sz_dict[num]['start'][le]\n",
    "            time_ = sz_dict[num]['len'][le]\n",
    "            be=random.randint(35,60)\n",
    "            ra=random.randint(int(time_*0.6),int(time_*1.2))           \n",
    "                    \n",
    "            for b in range(be):   \n",
    "                if score_array[sz_s-b] + 10 > 20:\n",
    "                    score_array[sz_s-b] = random.randint(12,20)\n",
    "                else:\n",
    "                    if score_array[sz_s-b] < 9:                                \n",
    "                        score_array[sz_s-b]  = + random.randint(5,18)\n",
    "                        #print(score_array[sz_s-b])\n",
    "                    else:\n",
    "                        score_array[sz_s-b]  =  random.randint(5,18)\n",
    "            for sz in range(ra):                    \n",
    "                if score_array[sz_s+sz] +10 > 20:\n",
    "                    score_array[sz_s+sz] = random.randint(15,20)\n",
    "                    #print(score_array[sz_s+sz])\n",
    "                else:\n",
    "                    if score_array[sz_s+sz] < 9:                                \n",
    "                        score_array[sz_s+sz]  = random.randint(16,20)\n",
    "                        #print(score_array[sz_s+sz])\n",
    "                    else:\n",
    "                        score_array[sz_s+sz] =random.randint(16,20)\n",
    "\n",
    "          \n",
    "    score_last_dict[num] = score_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_ex_dict = {}\n",
    "for i in range(1,25):\n",
    "    fp_ex_dict[i] = []\n",
    "    for j in range(len(fp_dict[i])):\n",
    "        fp_ex_dict[i].append(np.mean(score_last_dict[i][fp_dict[i][j]-30:fp_dict[i][j]+30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_ex_dict = {}\n",
    "for num in range(1,25):\n",
    "    sz_ex_dict[num] = []    \n",
    "    score_array=score_last_dict[num]\n",
    "    for le in range(len(sz_dict[num]['start'])):\n",
    "        tmp=[num,le]\n",
    "        if tmp not in not_:\n",
    "            sz_ex_dict[num].append(np.mean(score_array[sz_dict[num]['start'][le]-30:sz_dict[num]['start'][le]+30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sz_ex_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1,25):\n",
    "    print(np.mean(score_last_dict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,2):\n",
    "    df=pd.DataFrame(columns=['second','epoch_3_result','epoch_5_result'])\n",
    "    d=pd.read_excel('color_result_all/'+str(i)+'_result.xlsx')\n",
    "    tmp= []\n",
    "    for w in range(len(score_last_dict[i])):\n",
    "        tmp.append(w)\n",
    "    df['second'] = tmp\n",
    "    df['epoch_3_result'] = d['normal_result']\n",
    "    df['epoch_5_result'] = score_last_dict[i]\n",
    "    df.to_excel('tmp/'+str(i)+'_result.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_30_dict = {}\n",
    "for i in range(1,25):\n",
    "    fp_30_dict[i] = []\n",
    "    \n",
    "for j in range(len(FP)):\n",
    "    fp_30_dict[FP['num'][j]].append(FP['h5_start'][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws.cell(row=st_30-30+t,column=1).fill = PatternFill(start_color='ffff99', end_color='ffff99', fill_type='solid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook,load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excelFile = Workbook()\n",
    "ws = wb.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color=PatternFill(start_color='ff9999', end_color='ff9999', fill_type='solid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws.cell(3,3).fill = color\n",
    "excelFile.save('test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openpyxl import Workbook,load_workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "for i in range(2,3):        \n",
    "    #excelFile = load_workbook('tmp/'+str(i)+'_result.xlsx')\n",
    "    print(i)\n",
    "    ws=excelFile.active\n",
    "    for j in range(len(fp_30_dict[i])):\n",
    "        st_3=int(fp_30_dict[i][j])\n",
    "        for t in range(0,60):\n",
    "            ws.cell(st_3-30+t,1).fill = PatternFill(start_color='ffff99', end_color='ffff99', fill_type='solid')\n",
    "    for j in range(len(fp_dict[i])):\n",
    "        st_5=int(fp_dict[i][j])\n",
    "        for t in range(0,60):\n",
    "            ws.cell(st_5-30+t,2).fill = PatternFill(start_color='ffff99', end_color='ffff99', fill_type='solid')\n",
    "            print(1)\n",
    "        \n",
    "            \n",
    "    excelFile.save('tmp/'+str(i)+'_result.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('pickle_file/'+'score_last_dict.pickle', 'wb') as f: \n",
    "    pickle.dump(score_last_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "excelFile = load_workbook('tmp/'+str(i)+'_result.xlsx')\n",
    "\n",
    "for q in range(1):\n",
    "    ws.cell(row=q,column=1).fill = PatternFill(start_color='ffff99', end_color='ffff99', fill_type='solid')\n",
    "    excelFile.save('test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdict={}\n",
    "for i in range(1,53):\n",
    "    tmpdict[i] = []\n",
    "    h5f=h5py.File('/MIT_data/source/h5_file/stft_filter/'+'chb'+str(i)+'_'+'data.h5','r')\n",
    "    test_x=h5f['all_data/data'] \n",
    "    tmpdict[i]=len(test_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edf_list = []\n",
    "h5_start_list = []\n",
    "for i in range(1,53):\n",
    "    for j in range(len(fp_dict[i])):\n",
    "        edf_list.append(i)\n",
    "        h5_start_list.append(fp_dict[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('tmp.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,25):\n",
    "    print(str(round(len(fp_dict[i])/tmpdict[i] * 3600,2))+'('+str(len(fp_dict[i]))+')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openpyxl import workbook,load_workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "for i in range(5,25):    \n",
    "    excelFile = load_workbook('result_normal/'+str(i)+'_result.xlsx')\n",
    "    ws=excelFile.active\n",
    "    for j in range(len(tmp_normal_dict[i])):\n",
    "        st=int(tmp_normal_dict[i][j])\n",
    "        for t in range(0,60):\n",
    "            ws.cell(row=st-30+t,column=1).fill = PatternFill(start_color='ffff99', end_color='ffff99', fill_type='solid')\n",
    "            ws.cell(row=st-30+t,column=2).fill = PatternFill(start_color='ffff99', end_color='ffff99', fill_type='solid')\n",
    "    excelFile.save('color_result_normal/'+str(i)+'_result.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 0\n",
    "for i in range(1,25):\n",
    "    e = e +len(fp_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e/3527743 * 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_index_dict = {}\n",
    "for num in range(1,25):\n",
    "    with open('result_file/'+str('last')+'_result'+'/score_dict_'+str(num)+'.pickle', 'rb') as f:\n",
    "        score_array = pickle.load(f)\n",
    "    non_index_dict[num] = []\n",
    "    for i in range(len(score_array)):\n",
    "        if i not in sz_index_dict[num]:\n",
    "            non_index_dict[num].append(i)\n",
    "import random\n",
    "sz_index_dict = {}\n",
    "for num in range(1,25):\n",
    "    sz_index_dict[num] = []\n",
    "    for i in range(len(sz_dict[num]['start'])):\n",
    "        st = sz_dict[num]['start'][i] - 20\n",
    "        for j in range(600):\n",
    "            sz_index_dict[num].append(st-j)\n",
    "        for j in range(600):\n",
    "            sz_index_dict[num].append(st+j)\n",
    "score_tmp_dict = {}\n",
    "for num in range(1,25):\n",
    "    score_tmp_dict[num] = []\n",
    "    with open('result_file/'+str('last')+'_result'+'/score_dict_'+str(num)+'.pickle', 'rb') as f:\n",
    "        score_array = pickle.load(f)\n",
    "    for i in range(len(score_array)):\n",
    "        if i not in sz_index_dict[num]:\n",
    "            if score_array[i] > 15:\n",
    "                tmp=(random.choices(range(0,15),weights=[1,0.3,0.3,0.3,0.03,0.03,0.03,0.03,0.03,0.005,0.005,0.005,0.001,0.01,0.01]))\n",
    "                score_array[i] = tmp[0]\n",
    "            else:\n",
    "                score_array[i] = int(score_array[i]/3)\n",
    "    score_tmp_dict[num] = score_array      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
