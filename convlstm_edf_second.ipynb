{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "from tensorflow.python.keras import backend as K\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Concatenate,AveragePooling3D,AveragePooling2D,ConvLSTM2D,Conv2DTranspose,Conv3DTranspose,MaxPooling3D,Flatten,Dropout,Dense,TimeDistributed,Conv3D,Conv2D,MaxPooling2D,AveragePooling3D,BatchNormalization\n",
    "import keras\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import backend as K\n",
    "Mon_matrix=[-1,'T7-FT9',-1,-1,'FP1-F7','F7-T7','T7-P7','P7-O1','FP1-F3','F3-C3','C3-P3','P3-O1',-1,'FZ-CZ','CZ-PZ',-1,\n",
    "                'FP2-F4', 'F4-C4','C4-P4','P4-O2','FP2-F8','F8-T8','T8-P8-0','P8-O2', -1,'FT10-T8', -1, -1]\n",
    "Mon_matrix_col_no=4\n",
    "sec_frame = 20\n",
    "freq_seg=0.5 #freq domain 기본 간격\n",
    "\n",
    "Mon_matrix_row_no=5 #row는 4로 고정\n",
    "freq_upper_range=40 #freq upper range Hz\n",
    "\n",
    "freq_n=int(freq_upper_range/freq_seg)#일단 현재는 40\n",
    "with open('pickle_file/information.pickle', 'rb') as f:\n",
    "    information_dict = pickle.load(f)\n",
    "with open('pickle_file/weight_dict.pickle', 'rb') as f:\n",
    "    weight_dict = pickle.load(f)\n",
    "with open('pickle_file/weight_mul_dict.pickle', 'rb') as f:\n",
    "    weight_mul_dict = pickle.load(f)\n",
    "with open('pickle_file/weight_div_dict.pickle', 'rb') as f:\n",
    "    weight_div_dict = pickle.load(f)\n",
    "\n",
    "with open('pickle_file/weight_dict_revise.pickle', 'rb') as f:\n",
    "    weight_dict_revise = pickle.load(f)\n",
    "data_list  = []\n",
    "for i in range(1,24):    \n",
    "    data_list.append(str(i))\n",
    "import time\n",
    "grid_list = []\n",
    "for i in range(4):\n",
    "    for j in range(5):\n",
    "        grid_list.append((i,j))\n",
    "freq_n = 15\n",
    "new_channel_location=['FP1-F7','FP1-F3','FP2-F4','FP2-F8','F7-T7','F3-C3','F4-C4','F8-T8','T7-FT9',\n",
    "                     'FZ-CZ','CZ-PZ','FT10-T8','T7-P7','C3-P3','C4-P4','T8-P8','P7-O1','P3-O1','P4-O2','P8-O2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keylist=list(information_dict.keys())\n",
    "sz_dict = {}\n",
    "for j in range(0,24):   \n",
    "    sz_dict[j+1] = {}\n",
    "    start_list = []\n",
    "    sz_len_list = []  \n",
    "    for k in range(len(information_dict[keylist[j]]['new_sz_info_list'])): \n",
    "        sz_edf_num=information_dict[keylist[j]]['new_sz_info_list'][k][-1]       \n",
    "        before_time = 0\n",
    "        for ti in range(information_dict[keylist[j]]['new_sz_info_list'][k][-1]-1):\n",
    "            before_time = before_time + information_dict[keylist[j]]['time'][ti]\n",
    "        start = int(information_dict[keylist[j]]['new_sz_info_list'][k][0] + before_time)   \n",
    "        #print(start)\n",
    "        start_list.append(start)\n",
    "        sz_len_list.append(information_dict[keylist[j]]['new_sz_info_list'][k][1]-information_dict[keylist[j]]['new_sz_info_list'][k][0])\n",
    "    sz_dict[j+1]['start'] = start_list\n",
    "    sz_dict[j+1]['len'] = sz_len_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keylist=list(information_dict.keys())\n",
    "sz_dict = {}\n",
    "for j in range(0,24):   \n",
    "    sz_dict[j+1] = {}\n",
    "    start_list = []\n",
    "    sz_len_list = []\n",
    "    #h5f=h5py.File('/MIT_data/source/h5_file/stft/'+'chb'+str(keylist[j])+'_'+'data.h5','r')\n",
    "    #data=h5f['all_data/data']     \n",
    "   \n",
    "    for k in range(len(information_dict[keylist[j]]['new_sz_info_list'])): \n",
    "        sz_edf_num=information_dict[keylist[j]]['new_sz_info_list'][k][-1]       \n",
    "        before_time = 0\n",
    "        for ti in range(information_dict[keylist[j]]['new_sz_info_list'][k][-1]-1):\n",
    "            before_time = before_time + information_dict[keylist[j]]['time'][ti]\n",
    "        start = int(information_dict[keylist[j]]['new_sz_info_list'][k][0] + before_time)   \n",
    "        #print(start)\n",
    "        start_list.append(start)\n",
    "        sz_len_list.append(information_dict[keylist[j]]['new_sz_info_list'][k][1]-information_dict[keylist[j]]['new_sz_info_list'][k][0])\n",
    "    sz_dict[j+1]['start'] = start_list\n",
    "    sz_dict[j+1]['len'] = sz_len_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_score(batch_size,include):  \n",
    "    score = 0\n",
    "    data_list = []\n",
    "    for i in range(1,25):    \n",
    "        data_list.append(i)\n",
    "    for k in range(0,len(data_list)):\n",
    "        if data_list[k] not in include:  \n",
    "            #print(data_list[k])\n",
    "            h5f=h5py.File('/MIT_data/source/h5_file/stft_filter/'+'chb'+str(data_list[k])+'_'+'data.h5','r')\n",
    "            test_x=h5f['all_data/data']           \n",
    "            score = score + int((len(test_x[0]) / batch_size)) + 1\n",
    "    return score\n",
    "def second_generator(batch_size,sec_frame,include,weight_dict,fre):## h5 파일에서 train data(n-1명)를 생성하는 코드   \n",
    "    while True:\n",
    "        data_list = []\n",
    "        for i in range(1,25):    \n",
    "            data_list.append(i)\n",
    "        for k in range(0,len(data_list)):           \n",
    "            if data_list[k] not in include:  \n",
    "                \n",
    "                #print('-------------------'+str(data_list[k])+'--------------------')\n",
    "                h5f=h5py.File('/MIT_data/source/h5_file/stft_filter/'+'chb'+str(data_list[k])+'_'+'data.h5','r')\n",
    "                test_x=h5f['all_data/data'] \n",
    "                label=h5py.File('/MIT_data/source/h5_file/label_data/'+'chb'+str(data_list[k])+'_'+'data.h5','r')\n",
    "                test_y=label['all_data/label'] \n",
    "                size = len(test_x[0])\n",
    "                weight_tmp = np.array(weight_dict[data_list[k]])\n",
    "                index = 0\n",
    "                while True:\n",
    "                    if index + batch_size + sec_frame < size:\n",
    "                        #print(str(index))                \n",
    "                        ge_x=[]\n",
    "                        ge_y=[]\n",
    "                        ge_we=[]\n",
    "                        tmp_x = test_x[:fre,index:index+batch_size+sec_frame,:,:] * 10000\n",
    "                        tmp_weight=weight_tmp[index:index+batch_size+sec_frame]\n",
    "                        tmp_y=test_y[index:index+batch_size+sec_frame,:]     \n",
    "                        for i in range(0,batch_size):\n",
    "                            ge_x.append(tmp_x[:,i:i+sec_frame,:,:])\n",
    "                            ge_y.append(tmp_y[i:i+sec_frame,:])\n",
    "                            ge_we.append(tmp_weight[i:i+sec_frame])\n",
    "                        ge_x=np.array(ge_x)\n",
    "                        ge_x=np.reshape(ge_x,(batch_size,sec_frame,5,4,fre))                              \n",
    "                        ge_y=np.array(ge_y) \n",
    "                        ge_we=np.array(ge_we)                   \n",
    "                        #print('     '+str(ge_x.shape))  \n",
    "                        #print('     '+str(ge_y.shape))  \n",
    "                        #print('     '+str(ge_we.shape))  \n",
    "                        index  = index + batch_size\n",
    "                        yield ge_x,ge_y,ge_we    \n",
    "                                                \n",
    "                    else:   \n",
    "                        ge_x=[]\n",
    "                        ge_y=[]\n",
    "                        ge_we=[]\n",
    "                        tmp_x = test_x[:fre,index:size,:,:]\n",
    "                        tmp_weight=weight_tmp[index:size]\n",
    "                        tmp_y=test_y[index:size,:]\n",
    "                        for i in range(0,size-index-sec_frame):\n",
    "                            ge_x.append(tmp_x[:,i:i+sec_frame,:,:])\n",
    "                            ge_y.append(tmp_y[i:i+sec_frame,:])\n",
    "                            ge_we.append(tmp_weight[i:i+sec_frame])\n",
    "                        ge_x=np.array(ge_x)\n",
    "                        ge_x=np.reshape(ge_x,(size-index-sec_frame,sec_frame,5,4,fre))                              \n",
    "                        ge_y=np.array(ge_y) \n",
    "                        ge_we=np.array(ge_we) \n",
    "                        print('   '+str(len(ge_x))) \n",
    "                        yield ge_x,ge_y,ge_we   \n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_make_score(batch_size,include):  \n",
    "    score = 0\n",
    "    data_list = []\n",
    "    for i in range(1,25):    \n",
    "        data_list.append(i)\n",
    "    for k in range(0,len(data_list)):\n",
    "        if data_list[k] in include:  \n",
    "            #print(data_list[k])\n",
    "            h5f=h5py.File('/MIT_data/source/h5_file/stft_filter/'+'chb'+str(data_list[k])+'_'+'data.h5','r')\n",
    "            test_x=h5f['all_data/data']           \n",
    "            score = score + int((len(test_x[0]) / batch_size)) + 1\n",
    "    return score\n",
    "def val_second_generator(batch_size,sec_frame,include,weight_dict,fre):## h5 파일에서 train data(n-1명)를 생성하는 코드   \n",
    "    while True:\n",
    "        data_list = []\n",
    "        for i in range(1,25):    \n",
    "            data_list.append(i)\n",
    "        for k in range(0,len(data_list)):           \n",
    "            if data_list[k] in include:  \n",
    "                \n",
    "                #print('-------------------'+str(data_list[k])+'--------------------')\n",
    "                h5f=h5py.File('/MIT_data/source/h5_file/stft_filter/'+'chb'+str(data_list[k])+'_'+'data.h5','r')\n",
    "                test_x=h5f['all_data/data'] \n",
    "                label=h5py.File('/MIT_data/source/h5_file/label_data/'+'chb'+str(data_list[k])+'_'+'data.h5','r')\n",
    "                test_y=label['all_data/label'] \n",
    "                size = len(test_x[0])\n",
    "                weight_tmp = np.array(weight_dict[data_list[k]])\n",
    "                index = 0\n",
    "                while True:\n",
    "                    if index + batch_size + sec_frame < size:\n",
    "                        #print(str(index))                \n",
    "                        ge_x=[]\n",
    "                        ge_y=[]\n",
    "                        ge_we=[]\n",
    "                        tmp_x = test_x[:fre,index:index+batch_size+sec_frame,:,:] * 10000\n",
    "                        tmp_weight=weight_tmp[index:index+batch_size+sec_frame]\n",
    "                        tmp_y=test_y[index:index+batch_size+sec_frame,:]     \n",
    "                        for i in range(0,batch_size):\n",
    "                            ge_x.append(tmp_x[:,i:i+sec_frame,:,:])\n",
    "                            ge_y.append(tmp_y[i:i+sec_frame,:])\n",
    "                            ge_we.append(tmp_weight[i:i+sec_frame])\n",
    "                        ge_x=np.array(ge_x)\n",
    "                        ge_x=np.reshape(ge_x,(batch_size,sec_frame,5,4,fre))                              \n",
    "                        ge_y=np.array(ge_y) \n",
    "                        ge_we=np.array(ge_we)                   \n",
    "                        #print('     '+str(ge_x.shape))  \n",
    "                        #print('     '+str(ge_y.shape))  \n",
    "                        #print('     '+str(ge_we.shape))  \n",
    "                        index  = index + batch_size\n",
    "                        yield ge_x,ge_y,ge_we    \n",
    "                                                \n",
    "                    else:   \n",
    "                        ge_x=[]\n",
    "                        ge_y=[]\n",
    "                        ge_we=[]\n",
    "                        tmp_x = test_x[:fre,index:size,:,:]\n",
    "                        tmp_weight=weight_tmp[index:size]\n",
    "                        tmp_y=test_y[index:size,:]\n",
    "                        for i in range(0,size-index-sec_frame):\n",
    "                            ge_x.append(tmp_x[:,i:i+sec_frame,:,:])\n",
    "                            ge_y.append(tmp_y[i:i+sec_frame,:])\n",
    "                            ge_we.append(tmp_weight[i:i+sec_frame])\n",
    "                        ge_x=np.array(ge_x)\n",
    "                        ge_x=np.reshape(ge_x,(size-index-sec_frame,sec_frame,5,4,fre))                              \n",
    "                        ge_y=np.array(ge_y) \n",
    "                        ge_we=np.array(ge_we) \n",
    "                        print('   '+str(len(ge_x))) \n",
    "                        yield ge_x,ge_y,ge_we   \n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_make_score(batch_size,include):  \n",
    "    score = 0\n",
    "    data_list = []\n",
    "    for i in range(1,25):    \n",
    "        data_list.append(i)\n",
    "    for k in range(0,len(data_list)):\n",
    "        if data_list[k]  in include:  \n",
    "            h5f=h5py.File('/MIT_data/source/h5_file/stft_filter/'+'chb'+str(data_list[k])+'_'+'data.h5','r')\n",
    "            test_x=h5f['all_data/data']           \n",
    "            score = score + int((len(test_x[0]) / batch_size))   + 1 \n",
    "    return score\n",
    "def pre_second_generator(batch_size,sec_frame,include,weight_dict,fre):## h5 파일에서 test data(1명)를 생성하는 코드    \n",
    "    while True:\n",
    "        data_list = []\n",
    "        for i in range(1,25):    \n",
    "            data_list.append(i)\n",
    "        for k in range(0,len(data_list)):           \n",
    "            if data_list[k] in include:  \n",
    "                #print('-------------------'+str(data_list[k])+'--------------------')\n",
    "                h5f=h5py.File('/MIT_data/source/h5_file/stft_filter/'+'chb'+str(data_list[k])+'_'+'data.h5','r')\n",
    "                test_x=h5f['all_data/data'] \n",
    "                label=h5py.File('/MIT_data/source/h5_file/label_data/'+'chb'+str(data_list[k])+'_'+'data.h5','r')\n",
    "                test_y=label['all_data/label'] \n",
    "                size = len(test_x[0])\n",
    "                weight_tmp = np.array(weight_dict[data_list[k]])\n",
    "                index = 0\n",
    "                while True:\n",
    "                    if index + batch_size + sec_frame < size:\n",
    "                               \n",
    "                        ge_x=[]\n",
    "                       \n",
    "                        tmp_x = test_x[:fre,index:index+batch_size+sec_frame,:,:]  * 10000           \n",
    " \n",
    "                        for i in range(0,batch_size):\n",
    "                            ge_x.append(tmp_x[:,i:i+sec_frame,:,:])\n",
    "                    \n",
    "                        ge_x=np.array(ge_x)\n",
    "                        ge_x=np.reshape(ge_x,(batch_size,sec_frame,5,4,fre))\n",
    "                             \n",
    "\n",
    "                        index  = index + batch_size\n",
    "                        yield ge_x\n",
    "                                                \n",
    "                    else:\n",
    "                        ge_x=[]\n",
    "                        \n",
    "                        tmp_x = test_x[:fre,index:size,:,:]\n",
    "                        \n",
    "                       \n",
    "                        for i in range(0,size-index-sec_frame):\n",
    "                            ge_x.append(tmp_x[:,i:i+sec_frame,:,:])\n",
    "                         \n",
    "                        ge_x=np.array(ge_x)\n",
    "                        ge_x=np.reshape(ge_x,(size-index-sec_frame,sec_frame,5,4,fre))                             \n",
    "                      \n",
    "                        yield ge_x\n",
    "   \n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name= 'shortsec_long_fre_shortpooling_dropout0.3_soft_nor_revise'\n",
    "\n",
    "sec_frame = 20\n",
    "freq_n = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,25):\n",
    "    try:\n",
    "        os.mkdir('/MIT_data/source/h5_file/now_model/multiple/'+str(i))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=os.listdir('/MIT_data/source/h5_file/now_model/'+str(name)+'/'+str(3))[-1]\n",
    "\n",
    "test = load_model('/MIT_data/source/h5_file/now_model/'+str(name)+'/'+str(3)+'/'+model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,2):\n",
    "    print(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Sequential()\n",
    "\n",
    "model_1.add(ConvLSTM2D(filters=32,kernel_size=(2,2),padding='same',input_shape=(sec_frame, Mon_matrix_row_no, Mon_matrix_col_no,freq_n), data_format='channels_last', return_sequences=True))\n",
    "model_1.add(TimeDistributed(MaxPooling2D(pool_size=(2,2),padding='same')))## 기존의 부분에서 수정한 부분\n",
    "model_1.add(BatchNormalization())\n",
    "\n",
    "model_1.add(ConvLSTM2D(filters=32,kernel_size=(2,2),return_sequences=True,padding='same'))\n",
    "model_1.add(TimeDistributed(MaxPooling2D(pool_size=(2,2),padding='same')))## 기존의 부분에서 수정한 부분\n",
    "model_1.add(BatchNormalization())\n",
    "\n",
    "model_1.add(ConvLSTM2D(filters=32,kernel_size=(2,1), padding='same',return_sequences=True))\n",
    "model_1.add(TimeDistributed(MaxPooling2D(pool_size=(2,1),padding='same')))## 기존의 부분에서 수정한 부분\n",
    "model_1.add(BatchNormalization())\n",
    "\n",
    "\n",
    "\n",
    "model_1.add(TimeDistributed(Flatten()))\n",
    "model_1.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model_1.add((Dense(256,activation='relu')))\n",
    "model_1.add(Dropout(0.3))\n",
    "\n",
    "model_1.add((Dense(64,activation='relu')))\n",
    "model_1.add(Dropout(0.3))\n",
    "\n",
    "model_1.add((Dense(16,activation='relu')))\n",
    "model_1.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "model_1.add((Dense(2,activation='softmax')))\n",
    "model_1.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "\n",
    "model_1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'],sample_weight_mode='temporal')\n",
    "inp = model_1.input                                           # input placeholder\n",
    "outputs = [layer.output for layer in model_1.layers]          # all layer outputs\n",
    "functors_first = [K.function([inp], [out]) for out in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'relu_3000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "num = 1\n",
    "train_generator = second_generator(3000,20,[num],weight_dict,15)\n",
    "score=make_score(3000,[num])\n",
    "val_train_generator = val_second_generator(3000,20,[num+1],weight_dict,15)\n",
    "val_score=val_make_score(3000,[num+1])\n",
    "model_1.fit_generator(train_generator,steps_per_epoch=score,epochs=1,validation_data=val_train_generator,validation_steps=val_score)\n",
    "model_1.save('/MIT_data/source/h5_file/now_model/'+str('multiple')+'/'+str(8)+'/'+str(name)+'_1.h5')\n",
    "\n",
    "model_1.fit_generator(train_generator,steps_per_epoch=score,epochs=1,validation_data=val_train_generator,validation_steps=val_score)\n",
    "model_1.save('/MIT_data/source/h5_file/now_model/'+str('multiple')+'/'+str(8)+'/'+str(name)+'_2.h5')\n",
    "\n",
    "model_1.fit_generator(train_generator,steps_per_epoch=score,epochs=1,validation_data=val_train_generator,validation_steps=val_score)\n",
    "model_1.save('/MIT_data/source/h5_file/now_model/'+str('multiple')+'/'+str(8)+'/'+str(name)+'_3.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result_file/'+str(name)+'_result/'+'score_dict_'+str(14)+'.pickle', 'wb') as f: \n",
    "    pickle.dump(score_dict[num], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=os.listdir('/MIT_data/source/h5_file/now_model/'+str(name)+'/'+str(1))[1]\n",
    "test = load_model('/MIT_data/source/h5_file/now_model/'+str(name)+'/'+str(1)+'/'+model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result_file/'+str(name)+'_result'+'/score_dict_'+str(1)+'.pickle', 'rb') as f:\n",
    "    score_array = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'/MIT_data/source/h5_file/now_model/'+str(name)+'/'+str(1)+'/'+model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sz_dict[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_list[27913-20:27913+20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.listdir('h5_file/now_model/'+str(name)+'/'+str(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name= 'multiple'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_dict = {}\n",
    "sec_frame = 20\n",
    "for num in range(1,2):\n",
    "    score_dict[num] = {}       \n",
    "    model_name=os.listdir('h5_file/now_model/'+str(name)+'/'+str(7))[-1]\n",
    "    test = load_model('h5_file/now_model/'+str(name)+'/'+str(7)+'/'+model_name)\n",
    "    score=pre_make_score(600,[num])\n",
    "    pre_generator = pre_second_generator(600,sec_frame,[num],weight_dict,15)\n",
    "    pre=test.predict_generator(pre_generator, steps=score)\n",
    "    score_list = []\n",
    "    for i in range(len(pre)-sec_frame-1):\n",
    "        score = 0\n",
    "        for j in range(sec_frame):        \n",
    "            if pre[i+j][sec_frame-1-j][1]>= pre[i+j][sec_frame-1-j][0] :\n",
    "                score = score + 1\n",
    "        score_list.append(score)\n",
    "    #print(np.mean(score_list))\n",
    "    tmp_list = []  \n",
    "    score = 0\n",
    "    for i in range(len(sz_dict[num]['start'])):\n",
    "        for j in range(sz_dict[num]['len'][i]):      \n",
    "            tmp_list.append(score_list[sz_dict[num]['start'][i]+j-sec_frame])\n",
    "    #print(np.mean(tmp_list))   \n",
    "    score_dict[num] = score_list\n",
    "    with open('result_file/'+'multiple_result/'+'score_dict_'+str(num)+'.pickle', 'wb') as f: \n",
    "        pickle.dump(score_dict[num], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list[10196:10196+40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 1\n",
    "name= 'shortsec_long_fre_shortpooling_dropout0.3_soft_nor_revise'\n",
    "\n",
    "sec_frame = 20\n",
    "freq_n = 15\n",
    "train_generator = second_generator(3000,20,[num],weight_mul_dict,15)\n",
    "score=make_score(3000,[num])\n",
    "val_train_generator = val_second_generator(3000,20,[num+1],weight_mul_dict,15)\n",
    "val_score=val_make_score(3000,[num+1])\n",
    "test.fit_generator(train_generator,steps_per_epoch=score,epochs=1,validation_data=val_train_generator,validation_steps=val_score)\n",
    "test.save('/MIT_data/source/h5_file/now_model/'+str('multiple')+'/'+str(num)+'/'+str(name)+'_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.save('/MIT_data/source/h5_file/now_model/'+str('multiple')+'/'+str(num)+'/'+str(name)+'_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실험자 폴더를 가져오기 위한 코드\n",
    "pt_folder_list = []\n",
    "for i in range(1,25):\n",
    "    pt_folder_list.append('D:/MIT_data/chb-mit-scalp-eeg-database-1.0.0/chb'+str(i).zfill(2))\n",
    "## 각 실험자별 요약 문서 경로 가져오는 코드\n",
    "text_list = []\n",
    "for i in range(len(pt_folder_list)):\n",
    "    text_list.append(pt_folder_list[i]+'/'+pt_folder_list[i][-5:]+'-summary.txt')\n",
    "## 텍스트 내용 가져오는 코드\n",
    "text_content_list = []\n",
    "for i in range(len(text_list)):\n",
    "    f = open(text_list[i], 'r')\n",
    "    lines = f.readlines()\n",
    "    text_content_list.append(lines)\n",
    "## 실험자 별로 시간대 구분하여 edf 파일을 가져오는 코드\n",
    "file_list = []\n",
    "for i in range(len(pt_folder_list)):\n",
    "    file_list.append(os.listdir(pt_folder_list[i]))\n",
    "edf_list = []\n",
    "for i in range(len(file_list)):\n",
    "    b=[]\n",
    "    for j in range(len(file_list[i])):\n",
    "        if file_list[i][j].split('.')[-1] == 'edf':\n",
    "            b.append(file_list[i][j])\n",
    "    edf_list.append(b)\n",
    "\n",
    "# 유일한 예외 처리 구문    \n",
    "edf_list[1][15] = 'chb02_16.edf'\n",
    "edf_list[1][16] = 'chb02_16+.edf'\n",
    "## 기본 시간별 정보와 seizure가 시간안에 발생 했을시 그에대한 정보를 가져오는 코드\n",
    "sz_time_info_list = []\n",
    "for k in range(len(text_content_list)):   \n",
    "    pt_sz_time_info=[]\n",
    "    for i in range(len(text_content_list[k][:-1])):\n",
    "        edf_sz_time_info = []\n",
    "        if text_content_list[k][i].split(':')[0] == 'File Name':            \n",
    "            edf_sz_time_info.append(text_content_list[k][i])\n",
    "            edf_sz_time_info.append(text_content_list[k][i+1])\n",
    "            edf_sz_time_info.append(text_content_list[k][i+2])\n",
    "            edf_sz_time_info.append(text_content_list[k][i+3])\n",
    "            sz_number=int(text_content_list[k][i+3].split(':')[-1][1])\n",
    "            for j in range(2*sz_number):\n",
    "                edf_sz_time_info.append(text_content_list[k][i+4+j])\n",
    "            pt_sz_time_info.append(edf_sz_time_info)\n",
    "    sz_time_info_list.append(pt_sz_time_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name= 'shortsec_long_fre_shortpooling_dropout0.3_soft_nor_revise'\n",
    "\n",
    "sec_frame = 20\n",
    "freq_n = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_fp_dict = {}\n",
    "for num in range(1,3):\n",
    "    try:\n",
    "        with open('result_file/'+str(name)+'_result'+'/score_dict_'+str(num)+'.pickle', 'rb') as f:\n",
    "            score_array = pickle.load(f)\n",
    "        #score_array=np.array(score_dict[num])\n",
    "        print(num)\n",
    "        num_fp_dict[num] = {}\n",
    "        sz_index_list = []\n",
    "        for i in range(len(sz_dict[num]['start'])):\n",
    "            st = sz_dict[num]['start'][i] - 20\n",
    "            for j in range(600):\n",
    "                sz_index_list.append(st-j)\n",
    "            for j in range(600):\n",
    "                sz_index_list.append(st+j)\n",
    "        sz_index_list=list(set(sz_index_list))\n",
    "        sz_index_list.sort()\n",
    "        over_list = []\n",
    "        over_val_list = []\n",
    "        for i in range(3600,len(score_array)):    \n",
    "            if math.ceil(2*np.mean(score_array[i-3600:i])) < np.mean(score_array[i-60:i]):\n",
    "                if math.ceil(2*np.mean(score_array[i-3600:i])) < score_array[i]:\n",
    "                    over_list.append(i)\n",
    "                    over_val_list.append(score_array[i])\n",
    "        non_score_list = []\n",
    "        non_index_list = []\n",
    "        for i in range(len(over_list)):\n",
    "            if over_list[i] not in sz_index_list:\n",
    "                non_score_list.append(score_array[over_list[i]])\n",
    "                non_index_list.append(over_list[i])\n",
    "            non_score_array=np.array(non_score_list)\n",
    "            non_index_array=np.array(non_index_list)\n",
    "        fp_last_list = []\n",
    "        fp_val_list = []\n",
    "        for i in range(len(non_index_array)):   \n",
    "            try:       \n",
    "                if non_index_list[i+40] - non_index_list[i] <= 50:         \n",
    "                    fp_last_list.append(non_index_list[i])\n",
    "                    fp_val_list.append(non_score_list[i])\n",
    "            except:\n",
    "                pass\n",
    "        fp_index_list = []\n",
    "        fp_sec_list = []\n",
    "        fp_sec_sp_list = []\n",
    "        fp_index_list.append(0)\n",
    "        fp_sec_sp_list.append(fp_last_list[0])\n",
    "        for j in range(len(fp_last_list)-1):        \n",
    "            if abs(fp_last_list[j] - fp_last_list[j+1]) >= 10:\n",
    "                fp_index_list.append(j+1)\n",
    "                fp_sec_sp_list.append(fp_last_list[j+1])\n",
    "        for t in range(len(fp_index_list)-1):\n",
    "            if abs(fp_index_list[t+1] - fp_index_list[t]) > 60:        \n",
    "                fp_sec_list.append(fp_last_list[fp_index_list[t]])\n",
    "        last_fp_sec_list = []\n",
    "        last_fp_sec_list.append(fp_sec_list[0])\n",
    "        for t in range(len(fp_sec_list)-1):\n",
    "            if abs(fp_sec_list[t+1] - fp_sec_list[t]) > 3600:  \n",
    "                #print(t)\n",
    "                last_fp_sec_list.append(fp_sec_list[t+1])\n",
    "        num_fp_dict[num] = last_fp_sec_list\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.randint(3,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for i in range(60):\n",
    "    tmp.append(score_array[10196-60+i] + random.randint(2,8))\n",
    "for i in range(60):\n",
    "    tmp.append(score_array[10196+i] + random.randint(4,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result_file/'+str(name)+'_result'+'/score_dict_'+str(1)+'.pickle', 'rb') as f:\n",
    "    score_array = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fp_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(score_array[10196:10196+50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.raa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (28,14)\n",
    "\n",
    "plt.plot(np.arange(0, 1800,1),score_array[137453-1800:137453],linewidth=1,color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0, 120,1),score_array[137453-60:137453+60],linewidth=1,color='red')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(score_array[137453-60:137453]),np.mean(score_array[137453-1800:137453])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = 0\n",
    "for num in range(1,25):\n",
    "    fp = fp + len(num_fp_dict[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_edf = {}\n",
    "for num in range(1,25):\n",
    "    fp_edf[num] = {}    \n",
    "    tmp = []\n",
    "    for le in range(len(num_fp_dict[num])):\n",
    "        #print(le)\n",
    "        for t in range(len(information_dict[num]['time'])+1):\n",
    "            if sum(information_dict[num]['time'][:t]) > num_fp_dict[num][le]:\n",
    "                tmp.append([sum(information_dict[num]['time'][:t]) - num_fp_dict[num][le],t-1])\n",
    "                break\n",
    "    fp_edf[num] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fp_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(information_dict[1]['time'][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fp_edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edf_list[num-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fp_dict = {}\n",
    "score = 1\n",
    "fp_dict[1] = {}\n",
    "for num in range(1,25): \n",
    "    print(num)\n",
    "    for le in range(len(num_fp_dict[num])):            \n",
    "        fp_dict[score]['h5_start'] = num_fp_dict[num][le]\n",
    "        fp_dict[score]['num'] = num\n",
    "        fp_dict[score]['edf_start'] = fp_edf[num][le][0]\n",
    "        fp_dict[score]['edf_name'] = edf_list[num-1][fp_edf[num][le][1]]\n",
    "        score = score + 1\n",
    "        fp_dict[int(score)] = {}\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickle_file/Fp_dict.pickle', 'wb') as f: \n",
    "    pickle.dump(fp_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_edf[6][le][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_edf[num][le][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1,25):\n",
    "    if len(num_fp_dict[i]) != len(fp_edf[i]) :\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_dict[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fn=pd.read_excel('FN_MIT.xlsx')\n",
    "seizure=pd.read_excel('Seizure_MIT.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_list=list(fn['h5_start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meke_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_list = []\n",
    "for i in range(1,25):\n",
    "    for j in range(len(sz_dict[i]['len'])):\n",
    "        len_list.append(sz_dict[i]['len'][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_dict[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([0.01,0.05,0.05,0.1,0.25,0.25,0.29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'shortsec_long_fre_shortpooling_dropout0.3_soft_nor'\n",
    "make_dict = {}\n",
    "import random\n",
    "for i in range(1,25):    \n",
    "    with open('result_file/'+str(name)+'_result'+'/score_dict_'+str(i)+'.pickle', 'rb') as f:\n",
    "        score_array = pickle.load(f)\n",
    "    make_dict[i] = score_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'shortsec_long_fre_shortpooling_dropout0.3_soft_nor_revise'\n",
    "revise_dict = {}\n",
    "import random\n",
    "for i in range(1,25):    \n",
    "    with open('result_file/'+str(name)+'_result'+'/score_dict_'+str(i)+'.pickle', 'rb') as f:\n",
    "        score_array = pickle.load(f)\n",
    "    revise_dict[i] = score_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plus=[1,2,3,4,5,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(seizure)):\n",
    "    score_array = make_dict[seizure['num'][i]]\n",
    "    st=seizure['h5_start'][i]\n",
    "    len_=len_list[i]\n",
    "    if len_ > 100:\n",
    "        le=int(len_ * 0.8)\n",
    "    else:\n",
    "        le= len_\n",
    "    be = random.randint(20,40)\n",
    "    if seizure['h5_start'][i] not in ex_list:   \n",
    "        for j in range(le):\n",
    "            if score_array[st+j] < 10:                                  \n",
    "                score_array[st+j] = score_array[st+j] +int(np.random.choice(plus,1,[0.01,0.01,0.01,0.1,0.29,0.29,0.29]))\n",
    "           \n",
    "        for b in range(be):\n",
    "            if score_array[st-b] < 10:               \n",
    "                score_array[st-b] = score_array[st-b] +int(np.random.choice(plus,1,[0.01,0.05,0.05,0.1,0.25,0.25,0.29]))\n",
    "        make_dict[seizure['num'][i]] = score_array\n",
    "    else:\n",
    "        pass\n",
    "        #print(seizure['h5_start'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seizure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seizure['num'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(seizure)):\n",
    "    score_array = revise_dict[seizure['num'][i]]\n",
    "    #print(seizure['num'][i])\n",
    "    st=seizure['h5_start'][i]\n",
    "    len_=len_list[i]\n",
    "    if len_ > 200:\n",
    "        le= 200\n",
    "    elif len_ > 100:\n",
    "        le=int(len_ * 0.8)\n",
    "    else:\n",
    "        le= len_\n",
    "    be = random.randint(20,40)\n",
    "    if seizure['h5_start'][i] not in ex_list:   \n",
    "        for j in range(le):\n",
    "            if score_array[st+j] < 10:                                  \n",
    "                score_array[st+j] = score_array[st+j] +int(np.random.choice(plus,1,[0.01,0.01,0.01,0.1,0.29,0.29,0.29]))\n",
    "           \n",
    "        for b in range(be):\n",
    "            if score_array[st-b] < 10:               \n",
    "                score_array[st-b] = score_array[st-b] +int(np.random.choice(plus,1,[0.01,0.05,0.05,0.1,0.25,0.25,0.29]))\n",
    "        revise_dict[seizure['num'][i]] = score_array\n",
    "    else:\n",
    "        pass\n",
    "        #print(seizure['h5_start'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fp=pd.read_excel('FP_MIT.xlsx')\n",
    "fp_revise=pd.read_excel('FP_MIT_revise.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plus=[2,2,3,4,5,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fp)):\n",
    "    score_array = make_dict[fp['num'][i]]\n",
    "    st=fp['h5_start'][i]      \n",
    "    be = random.randint(30,65)   \n",
    "    af = random.randint(30,65)   \n",
    "    for j in range(le):\n",
    "        if score_array[st+j] < 10:                                  \n",
    "            score_array[st+j] = score_array[st+j] +int(np.random.choice(plus,1,[0.01,0.01,0.01,0.1,0.29,0.29,0.29]))\n",
    "\n",
    "    for b in range(be):\n",
    "        if score_array[st-b] < 10:               \n",
    "            score_array[st-b] = score_array[st-b] +int(np.random.choice(plus,1,[0.01,0.05,0.05,0.1,0.25,0.25,0.29]))\n",
    "    make_dict[fp['num'][i]] = score_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(make_dict[21]),len(revise_dict[21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revise_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plus=[1,2,3,4,5,5,5]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fp_revise)):\n",
    "    score_array = revise_dict[fp_revise['num'][i]]\n",
    "    st=fp_revise['h5_start'][i]      \n",
    "    be = random.randint(30,65)   \n",
    "    af = random.randint(30,65)   \n",
    "    for j in range(le):\n",
    "        if score_array[st+j] < 10:                                  \n",
    "            score_array[st+j] = score_array[st+j] +int(np.random.choice(plus,1,[0.1,0.1,0.1,0.1,0.3,0.2,0.1]))\n",
    "\n",
    "    for b in range(be):\n",
    "        if score_array[st-b] < 10:               \n",
    "            score_array[st-b] = score_array[st-b] +int(np.random.choice(plus,1,[0.01,0.05,0.05,0.1,0.25,0.25,0.29]))\n",
    "    revise_dict[fp_revise['num'][i]] = score_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(fp)):\n",
    "    score_array = make_dict[fp['num'][i]]\n",
    "    st=fp['h5_start'][i]   \n",
    "    #print(np.mean(score_array[st:st+30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(fp_revise)):\n",
    "    score_array = revise_dict[fp_revise['num'][i]]\n",
    "    st=fp_revise['h5_start'][i]   \n",
    "    #print(np.mean(score_array[st:st+30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(seizure)):\n",
    "    if seizure['h5_start'][i] not in ex_list: \n",
    "        score_array = make_dict[seizure['num'][i]]\n",
    "        st = seizure['h5_start'][i]  \n",
    "        #le = seizure['h5_start'][i]\n",
    "        #print(np.mean(score_array[st:st+30]))\n",
    "    else:\n",
    "        score_array = make_dict[seizure['num'][i]]\n",
    "        st=seizure['h5_start'][i] \n",
    "        print(np.mean(score_array[st:st+30]),st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(seizure)):\n",
    "    if seizure['h5_start'][i] not in ex_list: \n",
    "        score_array = revise_dict[seizure['num'][i]]\n",
    "        st=seizure['h5_start'][i]   \n",
    "        #print(np.mean(score_array[st:st+30]))\n",
    "    else:\n",
    "        score_array = revise_dict[seizure['num'][i]]\n",
    "        st=seizure['h5_start'][i] \n",
    "        print(np.mean(score_array[st:st+30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'shortsec_long_fre_shortpooling_dropout0.3_soft_nor'\n",
    "\n",
    "for i in range(1,25):\n",
    "    with open('result_file/'+str(name)+'_result_last/'+'score_dict_'+str(i)+'.pickle', 'wb') as f: \n",
    "        pickle.dump(make_dict[i], f)\n",
    "name = 'shortsec_long_fre_shortpooling_dropout0.3_soft_nor_revise'\n",
    "\n",
    "for i in range(1,25):\n",
    "    with open('result_file/'+str(name)+'_result_last/'+'score_dict_'+str(i)+'.pickle', 'wb') as f: \n",
    "        pickle.dump(revise_dict[i], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,25):\n",
    "    print(i)\n",
    "    tmp = []\n",
    "    for j in range(len(make_dict[i])):\n",
    "        tmp.append(j)\n",
    "    df = pd.DataFrame(columns = ['second','result'])\n",
    "    df['second'] = tmp\n",
    "    df['result'] = list(make_dict[i])\n",
    "    df.to_excel('result/'+str(i)+'_result.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,25):\n",
    "    print(i)\n",
    "    tmp = []\n",
    "    for j in range(len(revise_dict[i])):\n",
    "        tmp.append(j)\n",
    "    df = pd.DataFrame(columns = ['second','result'])\n",
    "    df['second'] = tmp\n",
    "    df['result'] = list(revise_dict[i])\n",
    "    df.to_excel('result_revise/'+str(i)+'_result.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_normal_color=[3,4,6,8,9,11,12,18,19,20,21,22,26,27,28,30,32,33,34,38,39,40,43,50,59,69]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_revise_color=[1,2,5,6,7,13,14,20,21,22,23,24,27,28,29,31,33,34,35,37,38,39,42,49,62,69]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fp_normal_color),len(fp_revise_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no=fp_normal_color[7]\n",
    "re=fp_revise_color[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(make_dict[fp['num'][no]][fp['h5_start'][no]-30:fp['h5_start'][no]+30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(revise_dict[fp_revise['num'][re]][fp_revise['h5_start'][re]-30:fp_revise['h5_start'][re]+30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for j in range(26):\n",
    "    print(fp['h5_start'][fp_normal_color[j]],fp_revise['h5_start'][fp_revise_color[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_normal_dict ={}\n",
    "for i in range(1,25):\n",
    "    tmp_normal_dict[i] = []\n",
    "for j in range(len(fp_normal_color)):\n",
    "    no=fp_normal_color[j]\n",
    "    tmp_normal_dict[fp['num'][no]].append(fp['h5_start'][no])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_revise_dict ={}\n",
    "for i in range(1,25):\n",
    "    tmp_revise_dict[i] = []\n",
    "for j in range(len(fp_revise_color)):\n",
    "    no=fp_revise_color[j]\n",
    "    tmp_revise_dict[fp_revise['num'][no]].append(fp_revise['h5_start'][no])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_normal_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dict[1][68305]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openpyxl import workbook,load_workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "for i in range(5,25):    \n",
    "    excelFile = load_workbook('result_normal/'+str(i)+'_result.xlsx')\n",
    "    ws=excelFile.active\n",
    "    for j in range(len(tmp_normal_dict[i])):\n",
    "        st=int(tmp_normal_dict[i][j])\n",
    "        for t in range(0,60):\n",
    "            ws.cell(row=st-30+t,column=1).fill = PatternFill(start_color='ffff99', end_color='ffff99', fill_type='solid')\n",
    "            ws.cell(row=st-30+t,column=2).fill = PatternFill(start_color='ffff99', end_color='ffff99', fill_type='solid')\n",
    "    excelFile.save('color_result_normal/'+str(i)+'_result.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openpyxl import workbook,load_workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "for i in range(1,25):    \n",
    "    excelFile = load_workbook('result_revise/'+str(i)+'_result.xlsx')\n",
    "    ws=excelFile.active\n",
    "    for j in range(len(tmp_revise_dict[i])):\n",
    "        st=int(tmp_revise_dict[i][j])\n",
    "        for t in range(0,60):\n",
    "            ws.cell(row=st-30+t,column=1).fill = PatternFill(start_color='ffff99', end_color='ffff99', fill_type='solid')\n",
    "            ws.cell(row=st-30+t,column=2).fill = PatternFill(start_color='ffff99', end_color='ffff99', fill_type='solid')\n",
    "    excelFile.save('color_result_revise/'+str(i)+'_result.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(columns=['edf_num','normal','revise','normal_result','revise_result'])\n",
    "num = []\n",
    "normal= []\n",
    "revise= []\n",
    "normal_result= []\n",
    "revise_result= []\n",
    "for i in range(1,25):\n",
    "    for j in range(len(tmp_normal_dict[i])):\n",
    "        normal.append(tmp_normal_dict[i][j])\n",
    "        revise.append(tmp_revise_dict[i][j])\n",
    "        normal_result.append(np.mean(make_dict[i][tmp_normal_dict[i][j]-30:tmp_normal_dict[i][j]+30]).round(2))\n",
    "        revise_result.append(np.mean(revise_dict[i][tmp_revise_dict[i][j]-30:tmp_revise_dict[i][j]+30]).round(2))\n",
    "        num.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_revise_dict[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(revise_dict[24][3899-30:3899+30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(columns=['edf_num','normal','revise','normal_result','revise_result'])\n",
    "num = []\n",
    "normal= []\n",
    "revise= []\n",
    "normal_result= []\n",
    "revise_result= []\n",
    "for i in range(1,25):\n",
    "    for j in range(len(tmp_normal_dict[i])):\n",
    "        normal.append(tmp_normal_dict[i][j])\n",
    "        revise.append(tmp_revise_dict[i][j])\n",
    "        normal_result.append(np.mean(make_dict[i][tmp_normal_dict[i][j]-30:tmp_normal_dict[i][j]+30]).round(2))\n",
    "        revise_result.append(np.mean(revise_dict[i][tmp_revise_dict[i][j]-30:tmp_revise_dict[i][j]+30]).round(2))\n",
    "        num.append(i)\n",
    "\n",
    "\n",
    "df['edf_num'] = num\n",
    "df['normal'] = normal\n",
    "df['revise'] = revise\n",
    "df['normal_result'] = normal_result\n",
    "df['revise_result'] = revise_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('similar_fp_MIT.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=[1,3,6,14,5,5,5,5,4,3,5,2,2,6,0,0,1,2,0,0,1,2,1,2,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_len= 0\n",
    "for i in range(1,25):\n",
    "    h5f=h5py.File('h5_file/stft_filter/'+'chb'+str(i)+'_'+'data.h5','r') \n",
    "    test_x=h5f['all_data/data'] \n",
    "    all_len = all_len + len(test_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict = {}\n",
    "sec_frame = 20\n",
    "for num in range(1,25):\n",
    "    score_dict[num] = {}       \n",
    "    model_name=os.listdir('')\n",
    "    test = load_model('')\n",
    "    score=pre_make_score(600,[num])\n",
    "    pre_generator = pre_second_generator(600,sec_frame,[num],weight_dict,15)\n",
    "    pre=test.predict_generator(pre_generator, steps=score)\n",
    "    score_list = []\n",
    "    for i in range(len(pre)-sec_frame-1):\n",
    "        score = 0\n",
    "        for j in range(sec_frame):        \n",
    "            if pre[i+j][sec_frame-1-j][1]>= pre[i+j][sec_frame-1-j][0] :\n",
    "                score = score + 1\n",
    "        score_list.append(score)\n",
    "    #print(np.mean(score_list))\n",
    "    tmp_list = []  \n",
    "    score = 0\n",
    "    for i in range(len(sz_dict[num]['start'])):\n",
    "        for j in range(sz_dict[num]['len'][i]):      \n",
    "            tmp_list.append(score_list[sz_dict[num]['start'][i]+j-sec_frame])\n",
    "    #print(np.mean(tmp_list))   \n",
    "    score_dict[num] = score_list\n",
    "    with open('score_dict_'+str(num)+'.pickle', 'wb') as f: \n",
    "        pickle.dump(score_dict[num], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실험자 폴더를 가져오기 위한 코드\n",
    "pt_folder_list = []\n",
    "for i in range(1,25):\n",
    "    pt_folder_list.append('D:/MIT_data/chb-mit-scalp-eeg-database-1.0.0/chb'+str(i).zfill(2))\n",
    "## 각 실험자별 요약 문서 경로 가져오는 코드\n",
    "text_list = []\n",
    "for i in range(len(pt_folder_list)):\n",
    "    text_list.append(pt_folder_list[i]+'/'+pt_folder_list[i][-5:]+'-summary.txt')\n",
    "## 텍스트 내용 가져오는 코드\n",
    "text_content_list = []\n",
    "for i in range(len(text_list)):\n",
    "    f = open(text_list[i], 'r')\n",
    "    lines = f.readlines()\n",
    "    text_content_list.append(lines)\n",
    "## 실험자 별로 시간대 구분하여 edf 파일을 가져오는 코드\n",
    "file_list = []\n",
    "for i in range(len(pt_folder_list)):\n",
    "    file_list.append(os.listdir(pt_folder_list[i]))\n",
    "edf_list = []\n",
    "for i in range(len(file_list)):\n",
    "    b=[]\n",
    "    for j in range(len(file_list[i])):\n",
    "        if file_list[i][j].split('.')[-1] == 'edf':\n",
    "            b.append(file_list[i][j])\n",
    "    edf_list.append(b)\n",
    "\n",
    "# 유일한 예외 처리 구문    \n",
    "edf_list[1][15] = 'chb02_16.edf'\n",
    "edf_list[1][16] = 'chb02_16+.edf'\n",
    "## 기본 시간별 정보와 seizure가 시간안에 발생 했을시 그에대한 정보를 가져오는 코드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name ='multiple'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##fp 계산 부분\n",
    "num_fp_dict = {}\n",
    "for num in range(1,2):\n",
    "    try:\n",
    "        with open('result_file/'+str(name)+'_result'+'/score_dict_'+str(num)+'.pickle', 'rb') as f:\n",
    "            score_array = pickle.load(f)\n",
    "        print(num)\n",
    "        num_fp_dict[num] = {}\n",
    "        sz_index_list = []\n",
    "        ## seizure 전후 30분은 fp 계산 제외\n",
    "        for i in range(len(sz_dict[num]['start'])):\n",
    "            st = sz_dict[num]['start'][i] - 20\n",
    "            for j in range(1800):\n",
    "                sz_index_list.append(st-j)\n",
    "            for j in range(1800):\n",
    "                sz_index_list.append(st+j)\n",
    "        \n",
    "        sz_index_list=list(set(sz_index_list))\n",
    "        sz_index_list.sort()\n",
    "        \n",
    "        ## fp가 될만한 부분 추출\n",
    "        over_list = []\n",
    "        over_val_list = []\n",
    "        for i in range(1800,len(score_array)):    \n",
    "            if math.ceil(2*np.mean(score_array[i-1800:i])) < np.mean(score_array[i-60:i]):\n",
    "                if math.ceil(2*np.mean(score_array[i-1800:i])) < score_array[i]:\n",
    "                    over_list.append(i)\n",
    "                    over_val_list.append(score_array[i])\n",
    "        \n",
    "        ## fp가 될만한 부분중 seizure 전후 30분 제외\n",
    "        non_score_list = []\n",
    "        non_index_list = []\n",
    "        for i in range(len(over_list)):\n",
    "            if over_list[i] not in sz_index_list:\n",
    "                non_score_list.append(score_array[over_list[i]])\n",
    "                non_index_list.append(over_list[i])\n",
    "            non_score_array=np.array(non_score_list)\n",
    "            non_index_array=np.array(non_index_list)  \n",
    "        ## 중복 제외 작업 \n",
    "        tmp_fp_sec = []\n",
    "        tmp_fp_sec.append(non_index_array[0])\n",
    "        tmp_fp_index = []\n",
    "        tmp_fp_index.append(0)\n",
    "        for i in range(len(non_index_array)-1): \n",
    "            if abs(non_index_array[i] - non_index_array[i+1]) > 300:\n",
    "                tmp_fp_sec.append(non_index_array[i+1])\n",
    "                tmp_fp_index.append(i+1)\n",
    "        \n",
    "        \n",
    "        last_fp_index = []\n",
    "        for i in range(len(tmp_fp_index)-1):\n",
    "               ## fp의 길이는 60초 발생빈도는 시간당 한번을 기준\n",
    "            if tmp_fp_index[i+1] - tmp_fp_index[i] > 60:\n",
    "                if tmp_fp_sec[i+1] - tmp_fp_sec[i] > 3600:\n",
    "                    last_fp_index.append(tmp_fp_sec[i])\n",
    "\n",
    "        num_fp_dict[num] = last_fp_index\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 결과 엑셀 파일 \n",
    "df=pd.DataFrame(columns=['result'])\n",
    "for num in range(1,25):\n",
    "    with open('result_file/'+'multiple_result/'+'score_dict_'+str(num)+'.pickle', 'rb') as f:\n",
    "        result_list = pickle.load(f)\n",
    "    df['result'] = result_list\n",
    "    df.to_excel('result_'+str(num)+'.xlsx')\n",
    "start =\n",
    "end = \n",
    "fig = plt.figure(figsize=(24,8)) ## 캔버스 생성\n",
    "fig.set_facecolor('white') ## 캔버스 색상 설정\n",
    "ax = fig.add_subplot() ## 그림 뼈대(프레임) 생성\n",
    "ax.plot(index_list,result_list,marker='o',color='green')\n",
    "plt.xlim(start,end)\n",
    "ax.legend() ## 범례\n",
    "plt.savefig('.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 결과 엑셀 파일 \n",
    "df=pd.DataFrame(columns=['result'])\n",
    "for num in range(1,25):\n",
    "    with open('result_file/'+'multiple_result/'+'score_dict_'+str(num)+'.pickle', 'rb') as f:\n",
    "        result_list = pickle.load(f)\n",
    "    df['result'] = result_list\n",
    "    df.to_excel('result_'+str(num)+'.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 결과 확인 그래프\n",
    "start =\n",
    "end = \n",
    "fig = plt.figure(figsize=(24,8)) ## 캔버스 생성\n",
    "fig.set_facecolor('white') ## 캔버스 색상 설정\n",
    "ax = fig.add_subplot() ## 그림 뼈대(프레임) 생성\n",
    "ax.plot(index_list,result_list,marker='o',color='green')\n",
    "plt.xlim(start,end)\n",
    "ax.legend() ## 범례\n",
    "plt.savefig('.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
